{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from ipywidgets import widgets, Checkbox, FloatSlider, IntSlider, IntRangeSlider, FloatRangeSlider, RadioButtons\n",
    "from IPython.core.display import clear_output, display\n",
    "\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "from image_pipeline import Operation, Color, Sobel, Magnitude, Direction, \\\n",
    "  Threshold, Combinator, FindLinesSlidingWindows, Annotate, ImagePipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Advanced Lane Finding Project\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Compute the camera calibration matrix and distortion coefficients given a set of chessboard images.\n",
    "* Apply a distortion correction to raw images.\n",
    "* Use color transforms, gradients, etc., to create a thresholded binary image.\n",
    "* Apply a perspective transform to rectify binary image (\"birds-eye view\").\n",
    "* Detect lane pixels and fit to find the lane boundary.\n",
    "* Determine the curvature of the lane and vehicle position with respect to center.\n",
    "* Warp the detected lane boundaries back onto the original image.\n",
    "* Output visual display of the lane boundaries and numerical estimation of lane curvature and vehicle position.\n",
    "\n",
    "---\n",
    "## Compute the camera calibration using chessboard images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# prepare object points, like (0,0,0), (1,0,0), (2,0,0) ....,(6,5,0)\n",
    "objp = np.zeros((6*9,3), np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Arrays to store object points and image points from all the images.\n",
    "objpoints = [] # 3d points in real world space\n",
    "imgpoints = [] # 2d points in image plane.\n",
    "\n",
    "# Make a list of calibration images\n",
    "images = glob.glob('camera_cal/calibration*.jpg')\n",
    "\n",
    "fig = plt.figure(figsize=(16, 10))\n",
    "\n",
    "# Step through the list and search for chessboard corners\n",
    "i = 0\n",
    "for fname in images:\n",
    "    img = cv2.imread(fname)\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Find the chessboard corners\n",
    "    ret, corners = cv2.findChessboardCorners(gray, (9,6),None)\n",
    "\n",
    "    # If found, add object points, image points\n",
    "    if ret == True:\n",
    "        objpoints.append(objp)\n",
    "        imgpoints.append(corners)\n",
    "\n",
    "        # Draw and display the corners\n",
    "        img = cv2.drawChessboardCorners(img, (9,6), corners, ret)\n",
    "        a = fig.add_subplot(math.ceil(float(len(images))/5.), 5, i+1)\n",
    "        plt.imshow(img)\n",
    "        i+=1\n",
    "        \n",
    "ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Example of undistorted images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sample_images = images[0:2]\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "\n",
    "i = 0\n",
    "for fname in sample_images:\n",
    "    name = os.path.basename(fname)\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    a = fig.add_subplot(math.ceil(float(len(sample_images))/2.)*2, 2, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.text(0, 805, name)\n",
    "    i+=1\n",
    "\n",
    "    a = fig.add_subplot(math.ceil(float(len(sample_images))/2.)*2, 2, i+1)\n",
    "    plt.imshow(dst)\n",
    "    plt.text(0, 805, \"{} undistorted\".format(name))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Calibrate test images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_images = glob.glob('test_images/*.jpg')\n",
    "dsts = []\n",
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "i = 0\n",
    "for fname in test_images:\n",
    "    name = os.path.basename(fname)\n",
    "    img = cv2.imread(fname)\n",
    "    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    dst = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    dsts.append(dst)\n",
    "    a = fig.add_subplot(math.ceil(float(len(test_images))/4.)*2, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.text(0, 880, name)\n",
    "    i+=1\n",
    "\n",
    "    a = fig.add_subplot(math.ceil(float(len(test_images))/4.)*2, 4, i+1)\n",
    "    plt.imshow(dst)\n",
    "    plt.text(0, 880, \"{} undistorted\".format(name))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Thresholded binary images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 12))\n",
    "\n",
    "def pipeline(img, s_thresh=(170, 255), sx_thresh=(30, 150)):\n",
    "    img = np.copy(img)\n",
    "    # Convert to HSV color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    h_channel = hsv[:,:,0]\n",
    "    l_channel = hsv[:,:,1]\n",
    "    s_channel = hsv[:,:,2]\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(s_channel, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    # Stack each channel\n",
    "    # Note color_binary[:, :, 0] is all 0s, effectively an all black image. It might\n",
    "    # be beneficial to replace this channel with something else.\n",
    "    color_binary = np.dstack(( np.zeros_like(sxbinary), sxbinary, s_binary))\n",
    "    \n",
    "    combined_binary = np.zeros_like(sxbinary)\n",
    "    combined_binary[(s_binary == 1) | (sxbinary == 1)] = 1\n",
    "    return combined_binary\n",
    "\n",
    "i = 0\n",
    "binaries = []\n",
    "for j, img in enumerate(dsts):\n",
    "    name = os.path.basename(test_images[j])\n",
    "    comb = pipeline(img)\n",
    "    binaries.append(comb)\n",
    "    a = fig.add_subplot(math.ceil(float(len(dsts))/4.)*2, 4, i+1)\n",
    "    plt.imshow(img)\n",
    "    plt.text(0, 880, name)\n",
    "    i+=1\n",
    "\n",
    "    a = fig.add_subplot(math.ceil(float(len(dsts))/4.)*2, 4, i+1)\n",
    "    plt.imshow(comb, cmap='gray')\n",
    "    plt.text(0, 880, \"{} undistorted\".format(name))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Perspective transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Finding a good source and destination coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "img = dsts[0]\n",
    "img_size = (img.shape[1], img.shape[0])\n",
    "\n",
    "src = np.float32([[233,700],\n",
    "                 [620,440],\n",
    "                 [660,440],\n",
    "                 [1075,700]])\n",
    "\n",
    "dst = np.float32([[300,719],\n",
    "                 [300,0],\n",
    "                 [900,0],\n",
    "                 [900,719]])\n",
    "\n",
    "a = fig.add_subplot(1, 2, 1)\n",
    "plt.imshow(img)\n",
    "a.plot(np.append(src[:,0],src[0,0]), np.append(src[:,1],src[0,1]), '-r')\n",
    "\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "warped = cv2.warpPerspective(img, M, img_size)\n",
    "\n",
    "a = fig.add_subplot(1, 2, 2)\n",
    "plt.imshow(warped)\n",
    "_ = a.plot(np.append(dst[:,0],dst[0,0]), np.append(dst[:,1],dst[0,1]), '-r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Find lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 48))\n",
    "img = dsts[0]\n",
    "nwindows = 9\n",
    "margin = 100\n",
    "minpix = 50\n",
    "\n",
    "img_size = (img.shape[1], img.shape[0])    \n",
    "\n",
    "# to cover same y-range as image\n",
    "ploty = np.linspace(0, img_size[1]-1, num=img_size[1])\n",
    "\n",
    "warpeds = []\n",
    "M = cv2.getPerspectiveTransform(src, dst)\n",
    "Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "\n",
    "i = 0\n",
    "sample = dsts\n",
    "for j, img in enumerate(sample):\n",
    "    \n",
    "    name = os.path.basename(test_images[j])\n",
    "    a = fig.add_subplot(math.ceil(float(len(dsts))/2.)*2, 2, i+1)\n",
    "    plt.imshow(img)\n",
    "    a.plot(src[:,0], src[:,1], 'or', ms=4)\n",
    "    a.plot(np.append(src[:,0],src[0,0]), np.append(src[:,1],src[0,1]), '-r')\n",
    "    plt.text(0, 880, name)\n",
    "    i+=1\n",
    "    \n",
    "    binary_warped = cv2.warpPerspective(binaries[j], M, img_size)\n",
    "    warpeds.append(binary_warped*255)\n",
    "    a = fig.add_subplot(math.ceil(float(len(dsts))/2.)*2, 2, i+1)\n",
    "\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    histogram = np.sum(binary_warped[int(warped.shape[0]/2):,:], axis=0)\n",
    "    \n",
    "    plt.imshow(binary_warped, cmap='gray')\n",
    "    a.plot(np.append(dst[:,0],dst[0,0]), np.append(dst[:,1],dst[0,1]), '-r')\n",
    "\n",
    "    # Subtract histogram values from max values so the histogram can be drawn\n",
    "    # at the bottom of the plot.\n",
    "    a.plot(binary_warped.shape[0] - histogram, '-', c='#00FFFF', lw=2)\n",
    "    plt.text(0, 880, \"{} warped\".format(name))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fitting a polynomial\n",
    "\n",
    "---\n",
    "\n",
    "#### Sliding Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 24))\n",
    "img = dsts[0]\n",
    "nwindows = 9\n",
    "margin = 100\n",
    "minpix = 50\n",
    "\n",
    "img_size = (img.shape[1], img.shape[0])    \n",
    "\n",
    "# to cover same y-range as image\n",
    "ploty = np.linspace(0, img_size[1]-1, num=img_size[1])\n",
    "\n",
    "i = 0\n",
    "sample = dsts\n",
    "for j, img in enumerate(sample):\n",
    "    \n",
    "    name = os.path.basename(test_images[j])\n",
    "    \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    binary_warped = cv2.warpPerspective(binaries[j], M, img_size)\n",
    "\n",
    "    a = fig.add_subplot(math.ceil(float(len(dsts))/2.), 2, i+1)\n",
    "\n",
    "    left_lane_inds = []\n",
    "    right_lane_inds = []\n",
    "    \n",
    "    histogram = np.sum(binary_warped[int(warped.shape[0]/2):,:], axis=0)\n",
    "\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    midpoint = np.int(histogram.shape[0]/2)\n",
    "    leftx_base = np.argmax(histogram[:midpoint])\n",
    "    rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "    # At this point, leftx_base and rightx_base should contain x position of each respective line.\n",
    "    \n",
    "    window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "\n",
    "    # Current positions to be updated for each window\n",
    "    leftx_current = leftx_base\n",
    "    rightx_current = rightx_base\n",
    "    \n",
    "    # Step through the windows one by one\n",
    "    for window in range(nwindows):\n",
    "        # Identify window boundaries in x and y (and right and left)\n",
    "        win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "        win_y_high = binary_warped.shape[0] - window*window_height\n",
    "        win_xleft_low = leftx_current - margin\n",
    "        win_xleft_high = leftx_current + margin\n",
    "        win_xright_low = rightx_current - margin\n",
    "        win_xright_high = rightx_current + margin\n",
    "        # Draw the windows on the visualization image\n",
    "        cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),(0,255,0), 2) \n",
    "        cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),(0,255,0), 2) \n",
    "        # Identify the nonzero pixels in x and y within the window\n",
    "        good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "        good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "        # Append these indices to the lists\n",
    "        left_lane_inds.append(good_left_inds)\n",
    "        right_lane_inds.append(good_right_inds)\n",
    "        # If you found > minpix pixels, recenter next window on their mean position\n",
    "        if len(good_left_inds) > minpix:\n",
    "            leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "        if len(good_right_inds) > minpix:        \n",
    "            rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "    # Concatenate the arrays of indices\n",
    "    left_lane_inds = np.concatenate(left_lane_inds)\n",
    "    right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "    # Extract left and right line pixel positions\n",
    "    leftx = nonzerox[left_lane_inds]\n",
    "    lefty = nonzeroy[left_lane_inds] \n",
    "    rightx = nonzerox[right_lane_inds]\n",
    "    righty = nonzeroy[right_lane_inds]\n",
    "    \n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # An array of y value from 0 to (image height - 1)\n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    \n",
    "    # Calculate x of each pixel position\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "\n",
    "    plt.plot(left_fitx, ploty, color='red', linewidth=3)\n",
    "    plt.plot(right_fitx, ploty, color='blue', linewidth=3)\n",
    "    \n",
    "    plt.imshow(out_img, cmap='gray')\n",
    "    \n",
    "    # Define y-value where we want radius of curvature\n",
    "    # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "    left_y_eval = np.max(lefty)\n",
    "    right_y_eval = np.max(righty)\n",
    "    left_curverad = ((1 + (2*left_fit[0]*left_y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "    right_curverad = ((1 + (2*right_fit[0]*right_y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "\n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(lefty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(righty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*left_y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*right_y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    # Now our radius of curvature is in meters\n",
    "\n",
    "    plt.text(0, 820, \"{} ({:.2f}m, {:.2f}m)\".format(name, left_curverad, right_curverad))\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Window search with convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(18, 24))\n",
    "    \n",
    "# window settings\n",
    "window_width = 50 \n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "cols = 2\n",
    "rows = math.ceil(float(len(dsts))/cols)\n",
    "\n",
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def find_window_centroids(image, window_width, window_height, margin):\n",
    "    \n",
    "    window_centroids = [] # Store the (left,right) window centroid positions per level\n",
    "    window = np.ones(window_width) # Create our window template that we will use for convolutions\n",
    "    \n",
    "    # First find the two starting positions for the left and right lane by using np.sum to get the vertical image slice\n",
    "    # and then np.convolve the vertical image slice with the window template \n",
    "    \n",
    "    # Sum quarter bottom of image to get slice, could use a different ratio\n",
    "    l_sum = np.sum(warped[int(3*warped.shape[0]/4):,:int(warped.shape[1]/2)], axis=0)\n",
    "    l_center = np.argmax(np.convolve(window,l_sum))-window_width/2\n",
    "    r_sum = np.sum(warped[int(3*warped.shape[0]/4):,int(warped.shape[1]/2):], axis=0)\n",
    "    r_center = np.argmax(np.convolve(window,r_sum))-window_width/2+int(warped.shape[1]/2)\n",
    "    \n",
    "    # Add what we found for the first layer\n",
    "    window_centroids.append((l_center,r_center))\n",
    "    \n",
    "    # Go through each layer looking for max pixel locations\n",
    "    for level in range(1,(int)(warped.shape[0]/window_height)):\n",
    "        # convolve the window into the vertical slice of the image\n",
    "        image_layer = np.sum(warped[int(warped.shape[0]-(level+1)*window_height):int(warped.shape[0]-level*window_height),:], axis=0)\n",
    "        conv_signal = np.convolve(window, image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        # Use window_width/2 as offset because convolution signal reference is at right side of window, not center of window\n",
    "        offset = window_width/2\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,warped.shape[1]))\n",
    "        l_center = np.argmax(conv_signal[l_min_index:l_max_index])+l_min_index-offset\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center+offset-margin,0))\n",
    "        r_max_index = int(min(r_center+offset+margin,warped.shape[1]))\n",
    "        r_center = np.argmax(conv_signal[r_min_index:r_max_index])+r_min_index-offset\n",
    "        # Add what we found for that layer\n",
    "        window_centroids.append((l_center,r_center))\n",
    "\n",
    "    return window_centroids\n",
    "\n",
    "i = 0\n",
    "sample = warpeds[0:1]\n",
    "for j, warped in enumerate(sample):\n",
    "    window_centroids = find_window_centroids(warped, window_width, window_height, margin)\n",
    "\n",
    "    # If we found any window centers\n",
    "    if len(window_centroids) > 0:\n",
    "\n",
    "        # Points used to draw all the left and right windows\n",
    "        l_points = np.zeros_like(warped)\n",
    "        r_points = np.zeros_like(warped)\n",
    "\n",
    "        # Go through each level and draw the windows \t\n",
    "        for level in range(0,len(window_centroids)):\n",
    "            # Window_mask is a function to draw window areas\n",
    "            l_mask = window_mask(window_width,window_height,warped,window_centroids[level][0],level)\n",
    "            r_mask = window_mask(window_width,window_height,warped,window_centroids[level][1],level)\n",
    "            # Add graphic points from window mask here to total pixels found \n",
    "            l_points[(l_points == 255) | ((l_mask == 1) ) ] = 255\n",
    "            r_points[(r_points == 255) | ((r_mask == 1) ) ] = 255\n",
    "\n",
    "        # Draw the results\n",
    "        template = np.array(r_points+l_points,np.uint8) # add both left and right window pixels together\n",
    "        zero_channel = np.zeros_like(template) # create a zero color channle \n",
    "        template = np.array(cv2.merge((zero_channel,template,zero_channel)),np.uint8) # make window pixels green\n",
    "        warpage = np.array(cv2.merge((warped,warped,warped)),np.uint8) # making the original road pixels 3 color channels\n",
    "        output = cv2.addWeighted(warpage, 1, template, 0.5, 0.0) # overlay the orignal road image with window results\n",
    "\n",
    "    # If no window centers found, just display orginal road image\n",
    "    else:\n",
    "        output = np.array(cv2.merge((warped,warped,warped)),np.uint8)\n",
    "        \n",
    "    a = fig.add_subplot(rows, cols, i+1)\n",
    "    # Display the final results\n",
    "    plt.imshow(output)\n",
    "    plt.title('window fitting results')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Refactor into a class\n",
    "\n",
    "This class should then be usable in other projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def example():\n",
    "    ip = ImagePipeline(input_color='bgr')\n",
    "#     ip.calibrate(glob.glob('camera_cal/calibration*.jpg'))\n",
    "    # If mtx and dist have been initialized, we can set them directly.\n",
    "    ip.mtx = mtx\n",
    "    ip.dist = dist\n",
    "\n",
    "    src = np.float32([[  100.,   719.],\n",
    "                      [  542.,   470.],\n",
    "                      [  738.,   470.],\n",
    "                      [ 1180.,   719.]])\n",
    "\n",
    "    dst = np.float32([[ 200.,  720.],\n",
    "                      [ 200.,    0.],\n",
    "                      [ 1080.,    0.],\n",
    "                      [ 1080.,  720.]])\n",
    "\n",
    "    ip.set_perspective(src, dst)\n",
    "\n",
    "    t1 = Threshold(trange=(0.3, 1.7))\n",
    "    t1.add(Color(['l', 'r', 'g']))\n",
    "    t1.add(Sobel('y', kernel=3))\n",
    "    t1.add(Sobel('x', kernel=3))\n",
    "    t1.add(Direction())\n",
    "\n",
    "    t2 = Threshold(trange=(30, 130))\n",
    "    t2.add(Color(['l', 'r', 'g']))\n",
    "    t2.add(Sobel('y', kernel=3))\n",
    "    t2.add(Magnitude())\n",
    "\n",
    "    ip.add(t1)\n",
    "    ip.add(t2)\n",
    "    c = Combinator(f=(lambda ths: np.where((ths[0] == 1) | (ths[1] == 1))))\n",
    "    ip.add(c)\n",
    "    \n",
    "    f = FindLinesSlidingWindows()\n",
    "    ip.add(f)\n",
    "    \n",
    "    a = Annotate(f)\n",
    "    ip.add(a)\n",
    "\n",
    "    img = cv2.imread(test_images[0])\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 12))\n",
    "    plt.imshow(ip.process(img))\n",
    "\n",
    "example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def mirror(img):\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    size = (undist.shape[1], undist.shape[0])\n",
    "    return np.concatenate((undist[:,:int(size[0]/2),:], np.fliplr(undist[:,:int(size[0]/2),:])), axis=1)\n",
    "    # Another side\n",
    "#     return np.concatenate((np.fliplr(undist[:,int(size[0]/2):,:]), undist[:,int(size[0]/2):,:]), axis=1)\n",
    "img = cv2.imread(test_images[0])\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "_ = plt.imshow(mirror(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def adjust_perspective(change=None):\n",
    "    src = np.float32(\n",
    "          [[src_x2_i.value[0], src_y2_i.value],\n",
    "           [src_x1_i.value[0], src_y1_i.value],\n",
    "           [src_x1_i.value[1], src_y1_i.value],\n",
    "           [src_x2_i.value[1], src_y2_i.value]])\n",
    "    dst = np.float32(\n",
    "          [[dst_x_i.value[0], dst_y_i.value[1]],\n",
    "           [dst_x_i.value[0], dst_y_i.value[0]],\n",
    "           [dst_x_i.value[1], dst_y_i.value[0]],\n",
    "           [dst_x_i.value[1], dst_y_i.value[1]]])\n",
    "    \n",
    "    img = cv2.imread('test_images/test_from_course.jpg')\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    undist = mirror(undist)\n",
    "    shape = undist.shape\n",
    "    im_size = (shape[1], shape[0])\n",
    "    \n",
    "    fig = plt.figure(figsize=(18, 6))\n",
    "\n",
    "    a = fig.add_subplot(1, 2, 1)\n",
    "    plt.imshow(undist)\n",
    "    a.plot(np.append(src[:,0],src[0,0]), np.append(src[:,1],src[0,1]), '-r')\n",
    "\n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    warped = cv2.warpPerspective(undist, M, im_size)\n",
    "\n",
    "    a = fig.add_subplot(1, 2, 2)\n",
    "    plt.imshow(warped)\n",
    "    _ = a.plot(np.append(dst[:,0],dst[0,0]), np.append(dst[:,1],dst[0,1]), '-r')\n",
    "    clear_output(True)\n",
    "    print(src)\n",
    "    print(dst)\n",
    "    \n",
    "\n",
    "# src_x1_i = IntRangeSlider(description='src x - top', min=0, max=im_size[0], step=1,value=[589, 690])\n",
    "# src_x2_i = IntRangeSlider(description='src x - bottom', min=0, max=im_size[0], step=1,value=[206, 1076])\n",
    "# src_y1_i = IntSlider(description='src y - top', min=0, max=im_size[1], step=1,value=470)\n",
    "# src_y2_i = IntSlider(description='src y - bottom', min=0, max=im_size[1], step=1,value=719)\n",
    "\n",
    "# dst_x_i = IntRangeSlider(description='dst x', min=0, max=im_size[0], step=1,value=[300, 962])\n",
    "# dst_y_i = IntRangeSlider(description='dst y', min=0, max=im_size[1], step=1,value=[0, 719])\n",
    "\n",
    "src_x1_i = IntRangeSlider(description='src x - top', min=0, max=1280, step=1,value=[542, 738])\n",
    "src_x2_i = IntRangeSlider(description='src x - bottom', min=0, max=1280, step=1,value=[100, 1180])\n",
    "src_y1_i = IntSlider(description='src y - top', min=0, max=720, step=1,value=470)\n",
    "src_y2_i = IntSlider(description='src y - bottom', min=0, max=720, step=1,value=719)\n",
    "\n",
    "# dst_x_i = IntRangeSlider(description='dst x', min=0, max=1280, step=1,value=[300, 980])\n",
    "dst_x_i = IntRangeSlider(description='dst x', min=0, max=1280, step=1,value=[120, 1160])\n",
    "dst_y_i = IntRangeSlider(description='dst y', min=0, max=720, step=1,value=[0, 720])\n",
    "\n",
    "src_x1_i.observe(adjust_perspective, 'value')\n",
    "src_x2_i.observe(adjust_perspective, 'value')\n",
    "src_y1_i.observe(adjust_perspective, 'value')\n",
    "src_y2_i.observe(adjust_perspective, 'value')\n",
    "\n",
    "dst_x_i.observe(adjust_perspective, 'value')\n",
    "dst_y_i.observe(adjust_perspective, 'value')\n",
    "\n",
    "display(src_x1_i, src_x2_i, src_y1_i, src_y2_i, dst_x_i, dst_y_i)\n",
    "adjust_perspective()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def adjust_thresholds(change=None):\n",
    "    \"\"\"\n",
    "    combiner values: \"dir\", \"mag\", None\n",
    "    \"\"\"\n",
    "    r=r_i.value\n",
    "    g=g_i.value\n",
    "    b=b_i.value\n",
    "    h=h_i.value\n",
    "    l=l_i.value\n",
    "    s=s_i.value\n",
    "    y=y_i.value\n",
    "    u=u_i.value\n",
    "    v=v_i.value\n",
    "    sobel_x=sobel_x_i.value\n",
    "    sobel_y=sobel_y_i.value\n",
    "    sobel_x_kernel=sobel_y_i.value\n",
    "    sobel_y_kernel=sobel_y_kernel_i.value\n",
    "    combiner=combiner_i.value\n",
    "    mag_or_sobel_range=mag_or_sobel_range_i.value\n",
    "    dir_range=dir_range_i.value\n",
    "    \n",
    "    ip = ImagePipeline(input_color='bgr')\n",
    "    # ip.calibrate(glob.glob('camera_cal/calibration*.jpg'))\n",
    "    ip.mtx = mtx\n",
    "    ip.dist = dist\n",
    "\n",
    "    src = np.float32([[  100.,   719.],\n",
    "                      [  542.,   470.],\n",
    "                      [  738.,   470.],\n",
    "                      [ 1180.,   719.]])\n",
    "\n",
    "    dst = np.float32([[ 120.,  720.],\n",
    "                      [ 120.,    0.],\n",
    "                      [ 1160.,    0.],\n",
    "                      [ 1160.,  720.]])\n",
    "\n",
    "    ip.set_perspective(src, dst)\n",
    "            \n",
    "    if combiner == 'dir':\n",
    "        t = Threshold(trange=dir_range)\n",
    "    else:\n",
    "        t = Threshold(trange=mag_or_sobel_range)\n",
    "        \n",
    "    cc = []\n",
    "    if r:\n",
    "        cc.append('r')\n",
    "    if g:\n",
    "        cc.append('g')\n",
    "    if b:\n",
    "        cc.append('b')\n",
    "    if h:\n",
    "        cc.append('h')\n",
    "    if l:\n",
    "        cc.append('l')\n",
    "    if s:\n",
    "        cc.append('s')\n",
    "    if y:\n",
    "        cc.append('y')\n",
    "    if u:\n",
    "        cc.append('u')\n",
    "    if v:\n",
    "        cc.append('v')\n",
    "    \n",
    "    t.add(Color(cc))\n",
    "    \n",
    "    if sobel_x:\n",
    "        t.add(Sobel('x', kernel=sobel_x_kernel))\n",
    "    \n",
    "    if sobel_y:\n",
    "        t.add(Sobel('y', kernel=sobel_y_kernel))\n",
    "        \n",
    "    if combiner == 'dir':\n",
    "        t.add(Direction())\n",
    "    elif combiner == 'mag':\n",
    "        t.add(Magnitude())\n",
    "            \n",
    "    ip.add(t)\n",
    "    c = Combinator(f=(lambda ths: np.where((ths[0] == 1))))\n",
    "    ip.add(c)\n",
    "    f = FindLinesSlidingWindows(always_recalculate=False)\n",
    "    ip.add(f)\n",
    "    \n",
    "    test_images = glob.glob('test_images/*.jpg')\n",
    "\n",
    "    fig = plt.figure(figsize=(18, 18))\n",
    "    i = 0\n",
    "    for fname in test_images:\n",
    "        name = os.path.basename(fname)\n",
    "        img = cv2.imread(fname)\n",
    "        \n",
    "        a = fig.add_subplot(math.ceil(float(len(test_images))/4.)*2, 4, i+1)\n",
    "        plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "        plt.text(0, 880, name)\n",
    "        i+=1\n",
    "\n",
    "        processed = ip.process(img)\n",
    "        a = fig.add_subplot(math.ceil(float(len(test_images))/4.)*2, 4, i+1)\n",
    "        plt.imshow(processed, cmap='gray')\n",
    "        plt.plot(f.left_fitx, f.ploty, color='red', linewidth=2)\n",
    "        plt.plot(f.right_fitx, f.ploty, color='blue', linewidth=2)\n",
    "        \n",
    "        binary_warped = processed\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = nonzero[0]\n",
    "        nonzerox = nonzero[1]\n",
    "        left_lane_inds = f.left_lane_inds\n",
    "        right_lane_inds = f.right_lane_inds\n",
    "        left_fitx = f.left_fitx\n",
    "        right_fitx = f.right_fitx\n",
    "        ploty = f.ploty\n",
    "        margin = f.subsequent_search_margin\n",
    "        \n",
    "        \n",
    "        # Create an image to draw on and an image to show the selection window\n",
    "        out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "        window_img = np.zeros_like(out_img)\n",
    "        # Color in left and right line pixels\n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 255, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 255, 255]\n",
    "\n",
    "        # Generate a polygon to illustrate the search window area\n",
    "        # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "        left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "        left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, ploty])))])\n",
    "        left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "        right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "        right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, ploty])))])\n",
    "        right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "\n",
    "        # Draw the lane onto the warped blank image\n",
    "        cv2.fillPoly(window_img, np.int_([left_line_pts]), (0,255, 0))\n",
    "        cv2.fillPoly(window_img, np.int_([right_line_pts]), (0,255, 0))\n",
    "        result = cv2.addWeighted(out_img, 1, window_img, 0.3, 0)\n",
    "        plt.imshow(result)\n",
    "        plt.plot(left_fitx, ploty, color='yellow')\n",
    "        plt.plot(right_fitx, ploty, color='yellow')\n",
    "        plt.xlim(0, 1280)\n",
    "        plt.ylim(720, 0)\n",
    "\n",
    "        plt.text(0, 880, \"{} processed\".format(name))\n",
    "        plt.text(640, 700, \"{:.2f}m {:.2f}m\".format(f.left_curverad_m, f.right_curverad_m), color='white', ha='center')\n",
    "        i+=1\n",
    "    clear_output(True)\n",
    "\n",
    "\n",
    "r_i = Checkbox(description='r', value=False)\n",
    "g_i = Checkbox(description='g', value=False)\n",
    "b_i = Checkbox(description='b', value=False)\n",
    "h_i = Checkbox(description='h', value=False)\n",
    "l_i = Checkbox(description='l', value=False)\n",
    "s_i = Checkbox(description='s', value=True)\n",
    "y_i = Checkbox(description='y', value=False)\n",
    "u_i = Checkbox(description='u', value=False)\n",
    "v_i = Checkbox(description='v', value=False)\n",
    "sobel_x_i = Checkbox(description='sobel x', value=True)\n",
    "sobel_y_i = Checkbox(description='sobel y', value=True)\n",
    "sobel_x_kernel_i = IntSlider(description='sobel x kernel', min=1, max=31, step=2, value=3)\n",
    "sobel_y_kernel_i = IntSlider(description='sobel y kernel', min=1, max=31, step=2, value=3)\n",
    "combiner_i = RadioButtons(description='combiner', options=[None, 'mag', 'dir'], value='mag')\n",
    "mag_or_sobel_range_i = IntRangeSlider(description='mag or sobel range', min=0, max=512, step=1, value=[8, 137])\n",
    "dir_range_i = FloatRangeSlider(description='dir range', min=0, max=np.pi / 2, step=0.01,value=[0.7, 1.3])\n",
    "\n",
    "r_i.observe(adjust_thresholds, 'value')\n",
    "g_i.observe(adjust_thresholds, 'value')\n",
    "b_i.observe(adjust_thresholds, 'value')\n",
    "h_i.observe(adjust_thresholds, 'value')\n",
    "l_i.observe(adjust_thresholds, 'value')\n",
    "s_i.observe(adjust_thresholds, 'value')\n",
    "y_i.observe(adjust_thresholds, 'value')\n",
    "u_i.observe(adjust_thresholds, 'value')\n",
    "v_i.observe(adjust_thresholds, 'value')\n",
    "sobel_x_i.observe(adjust_thresholds, 'value')\n",
    "sobel_y_i.observe(adjust_thresholds, 'value')\n",
    "sobel_x_kernel_i.observe(adjust_thresholds, 'value')\n",
    "sobel_y_kernel_i.observe(adjust_thresholds, 'value')\n",
    "combiner_i.observe(adjust_thresholds, 'value')\n",
    "mag_or_sobel_range_i.observe(adjust_thresholds, 'value')\n",
    "dir_range_i.observe(adjust_thresholds, 'value')\n",
    "\n",
    "display(r_i, g_i, b_i, h_i, l_i, s_i, y_i, u_i, v_i, sobel_x_i, sobel_y_i, sobel_x_kernel_i, sobel_y_kernel_i, combiner_i,\n",
    "        mag_or_sobel_range_i, dir_range_i)\n",
    "adjust_thresholds()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ip_final = ImagePipeline(input_color='bgr')\n",
    "\n",
    "src = np.float32([[  100.,   719.],\n",
    "                  [  542.,   470.],\n",
    "                  [  738.,   470.],\n",
    "                  [ 1180.,   719.]])\n",
    "\n",
    "dst = np.float32([[ 120.,  720.],\n",
    "                  [ 120.,    0.],\n",
    "                  [ 1160.,    0.],\n",
    "                  [ 1160.,  720.]])\n",
    "\n",
    "ip_final.set_perspective(src, dst)\n",
    "# ip_final.calibrate(glob.glob('camera_cal/calibration*.jpg'))\n",
    "ip_final.mtx = mtx\n",
    "ip_final.dist = dist\n",
    "\n",
    "t1 = Threshold(trange=(8, 137))\n",
    "t1.add(Color(['s']))\n",
    "t1.add(Sobel('y', kernel=3))\n",
    "t1.add(Sobel('x', kernel=3))\n",
    "t1.add(Magnitude())\n",
    "\n",
    "# t2 = Threshold(trange=(5, 47))\n",
    "# t2.add(Color(['s']))\n",
    "# t2.add(Sobel('y', kernel=19))\n",
    "# t2.add(Sobel('x', kernel=19))\n",
    "# t2.add(Magnitude())\n",
    "\n",
    "# t2 = Threshold(trange=(4, 130))\n",
    "# t2.add(Color(['s']))\n",
    "# t2.add(Sobel('y', kernel=3))\n",
    "\n",
    "ip_final.add(t1)\n",
    "# ip_final.add(t2)\n",
    "# c = Combinator(f=(lambda ths: np.where((ths[0] == 1) & (ths[1] == 1))))\n",
    "c = Combinator(f=(lambda ths: np.where((ths[0] == 1))))\n",
    "ip_final.add(c)\n",
    "f = FindLinesSlidingWindows(always_recalculate=True)\n",
    "ip_final.add(f)\n",
    "\n",
    "a = Annotate(f)\n",
    "ip_final.add(a)\n",
    "\n",
    "test_images = glob.glob('test_images/*.jpg')\n",
    "\n",
    "fig = plt.figure(figsize=(18, 10))\n",
    "i = 0\n",
    "for fname in test_images:\n",
    "    name = os.path.basename(fname)\n",
    "    img = cv2.imread(fname)\n",
    "\n",
    "    processed = ip_final.process(img)\n",
    "    a = fig.add_subplot(math.ceil(float(len(test_images))/4.), 4, i+1)\n",
    "    plt.imshow(processed)\n",
    "\n",
    "    plt.text(0, 880, \"{} processed\".format(name))\n",
    "    plt.text(0, 960, \"{:.2f}m\".format(f.left_curverad_m), color='red')\n",
    "    plt.text(1280, 960, \"{:.2f}m\".format(f.right_curverad_m), color='blue', horizontalalignment='right')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FindLinesSlidingWindows(Operation):\n",
    "    def __init__(self,\n",
    "                 xm_per_pix=3.7/720, ym_per_pix=30/720,\n",
    "                 # http://plannersweb.com/2013/09/wide-neighborhood-street-part-1/\n",
    "                 ok_lanes_dist_m_min=2.23,\n",
    "                 # https://www.google.com/webhp?sourceid=chrome-instant&rlz=1C1CHBF_enUS734US735&ion=1&espv=2&ie=UTF-8#q=standard+width+of+street+lane&*\n",
    "                 ok_lanes_dist_m_max=3.8,\n",
    "                 ok_curverad_diff_m_max=1600,\n",
    "                 alpha=0.5,\n",
    "                 nwindows=9,\n",
    "                 window_minpix=50,\n",
    "                 lane_minpix=50,\n",
    "                 subsequent_search_margin=100, always_recalculate=False):\n",
    "        self.type = TYPE_LINEAR\n",
    "        self.left_lane_inds = []\n",
    "        self.right_lane_inds = []\n",
    "        self.ploty = None\n",
    "        self.left_fit = None\n",
    "        self.right_fit = None\n",
    "        self.left_fitx = None\n",
    "        self.right_fitx = None\n",
    "        \n",
    "        self.left_curverad = None\n",
    "        self.right_curverad = None\n",
    "        self.left_curverad_m = None\n",
    "        self.right_curverad_m = None\n",
    "        self.xm_per_pix = xm_per_pix\n",
    "        self.ym_per_pix = ym_per_pix\n",
    "        \n",
    "        self.distance_to_center = None\n",
    "        self.distance_to_center_m = None\n",
    "        \n",
    "        # Initial search windows\n",
    "        self.windows = []\n",
    "        self.alpha = alpha\n",
    "        self.nwindows = nwindows\n",
    "        self.window_minpix = window_minpix\n",
    "        \n",
    "        # Subsequent search related\n",
    "        self.subsequent_search_margin = subsequent_search_margin\n",
    "        self.always_recalculate = always_recalculate\n",
    "        self.lane_minpix = lane_minpix\n",
    "        \n",
    "        # Check if lanes are reasonable\n",
    "        self.ok_lanes_dist_m_min = ok_lanes_dist_m_min\n",
    "        self.ok_lanes_dist_m_max = ok_lanes_dist_m_max\n",
    "        self.ok_curverad_diff_m_max = ok_curverad_diff_m_max\n",
    "            \n",
    "    def _calculate_fits(self, binary_warped):\n",
    "        margin = self.subsequent_search_margin\n",
    "        histogram = np.sum(binary_warped[int(binary_warped.shape[0]/2):,:], axis=0)\n",
    "\n",
    "        midpoint = np.int(histogram.shape[0]/2)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "        # At this point, leftx_base and rightx_base should contain x position of each respective line.\n",
    "        window_height = np.int(binary_warped.shape[0]/self.nwindows)\n",
    "\n",
    "        # Identify the x and y positions of all nonzero pixels in the image\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "\n",
    "        # Current positions to be updated for each window\n",
    "        leftx_current = leftx_base\n",
    "        rightx_current = rightx_base\n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(self.nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "            win_y_high = binary_warped.shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "            self.windows.append({\n",
    "                'win_y_low': win_y_low,\n",
    "                'win_y_high': win_y_high,\n",
    "                'win_xleft_low': win_xleft_low,\n",
    "                'win_xleft_high': win_xleft_high,\n",
    "                'win_xright_low': win_xright_low,\n",
    "                'win_xright_high': win_xright_high\n",
    "            })\n",
    "\n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \\\n",
    "                              (nonzerox >= win_xleft_low) & (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \\\n",
    "                               (nonzerox >= win_xright_low) & (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            self.left_lane_inds.append(good_left_inds)\n",
    "            self.right_lane_inds.append(good_right_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > self.minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > self.minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "            \n",
    "        if len(self.left_lane_inds) < self.lane_minpix or len(self.right_lane_inds) < self.lane_minpix:\n",
    "            self.left_lane_inds = []\n",
    "            self.right_lane_inds = []\n",
    "            self.windows = []\n",
    "            return self._reuse_fits(binary_warped)\n",
    "            \n",
    "        # Concatenate the arrays of indices\n",
    "        self.left_lane_inds = np.concatenate(self.left_lane_inds)\n",
    "        self.right_lane_inds = np.concatenate(self.right_lane_inds)\n",
    "\n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[self.left_lane_inds]\n",
    "        lefty = nonzeroy[self.left_lane_inds] \n",
    "        rightx = nonzerox[self.right_lane_inds]\n",
    "        righty = nonzeroy[self.right_lane_inds]\n",
    "        # binary_warped[nonzeroy[self.left_lane_inds], nonzerox[self.left_lane_inds]] would\n",
    "        # select all non-zero points. Remember that binary_warped is one dimensional.\n",
    "\n",
    "        # Fit a second order polynomial to each\n",
    "        self.left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        self.right_fit = np.polyfit(righty, rightx, 2)\n",
    "\n",
    "        # An array of y value from 0 to (image height - 1)\n",
    "        self.ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "\n",
    "        # Calculate x of each pixel y position\n",
    "        cur_left_fitx = self.left_fit[0]*self.ploty**2 + self.left_fit[1]*self.ploty + self.left_fit[2]\n",
    "        cur_right_fitx = self.right_fit[0]*self.ploty**2 + self.right_fit[1]*self.ploty + self.right_fit[2]\n",
    "        if self.left_fitx is None or self.always_recalculate:\n",
    "            self.left_fitx = cur_left_fitx\n",
    "            self.right_fitx = cur_right_fitx\n",
    "        else:\n",
    "            prev_left_fitx = np.copy(self.left_fitx)\n",
    "            prev_right_fitx = np.copy(self.right_fitx)\n",
    "            self.left_fitx = prev_left_fitx * (1 - self.alpha) + cur_left_fitx * self.alpha\n",
    "            self.right_fitx = prev_right_fitx * (1 - self.alpha) + cur_right_fitx * self.alpha\n",
    "            \n",
    "        return binary_warped\n",
    "        \n",
    "    def _reuse_fits(self, binary_warped):\n",
    "        nonzero = binary_warped.nonzero()\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        margin = self.subsequent_search_margin\n",
    "        self.left_lane_inds = ((nonzerox > (self.left_fit[0]*(nonzeroy**2) + \\\n",
    "                                            self.left_fit[1]*nonzeroy + self.left_fit[2] - margin)) & \\\n",
    "                               (nonzerox < (self.left_fit[0]*(nonzeroy**2) + self.left_fit[1]*nonzeroy + \\\n",
    "                                            self.left_fit[2] + margin))) \n",
    "        self.right_lane_inds = ((nonzerox > (self.right_fit[0]*(nonzeroy**2) + \\\n",
    "                                             self.right_fit[1]*nonzeroy + self.right_fit[2] - margin)) & \\\n",
    "                                (nonzerox < (self.right_fit[0]*(nonzeroy**2) + self.right_fit[1]*nonzeroy + \\\n",
    "                                             self.right_fit[2] + margin)))  \n",
    "\n",
    "        # Again, extract left and right line pixel positions\n",
    "        leftx = nonzerox[self.left_lane_inds]\n",
    "        lefty = nonzeroy[self.left_lane_inds] \n",
    "        rightx = nonzerox[self.right_lane_inds]\n",
    "        righty = nonzeroy[self.right_lane_inds]\n",
    "        # binary_warped[nonzeroy[self.left_lane_inds], nonzerox[self.left_lane_inds]] would\n",
    "        # select all non-zero points. Remember that binary_warped is one dimensional.\n",
    "        \n",
    "        # Fit a second order polynomial to each\n",
    "        self.left_fit = np.polyfit(lefty, leftx, 2)\n",
    "        self.right_fit = np.polyfit(righty, rightx, 2)\n",
    "        # Generate x and y values for plotting\n",
    "        self.ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "        self.left_fitx = self.left_fit[0]*self.ploty**2 + self.left_fit[1]*self.ploty + self.left_fit[2]\n",
    "        self.right_fitx = self.right_fit[0]*self.ploty**2 + self.right_fit[1]*self.ploty + self.right_fit[2]\n",
    "        return binary_warped\n",
    "    \n",
    "    def _lanes_are_reasonable(self):\n",
    "        distance = self.distance_between_lanes(self.left_fitx, self.right_fitx, self.xm_per_pix)\n",
    "        shorter = distance[np.where(distance < self.ok_lanes_dist_m_min)]\n",
    "        longer = distance[np.where(distance > self.ok_lanes_dist_m_max)]\n",
    "        if len(shorter) > 0 or len(longer) > 0:\n",
    "            return False\n",
    "        \n",
    "        if abs(self.left_curverad_m - self.right_curverad_m) > self.ok_curverad_diff_m_max:\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    def distance_between_lanes(self, leftx, rightx, xm_per_pix):\n",
    "        return (rightx - leftx) * xm_per_pix\n",
    "        \n",
    "    def calculate_curvature_radius(self, left_fit, right_fit, leftx, rightx, ploty, xm_per_pix, ym_per_pix):\n",
    "        # Define y-value where we want radius of curvature\n",
    "        # I'll choose the maximum y-value, corresponding to the bottom of the image\n",
    "        y_eval = np.max(ploty)\n",
    "        left_curverad = ((1 + (2*left_fit[0]*y_eval + left_fit[1])**2)**1.5) / np.absolute(2*left_fit[0])\n",
    "        right_curverad = ((1 + (2*right_fit[0]*y_eval + right_fit[1])**2)**1.5) / np.absolute(2*right_fit[0])\n",
    "        \n",
    "        # Fit new polynomials to x,y in world space\n",
    "        left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "        right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "        \n",
    "        # Calculate the new radii of curvature\n",
    "        left_curverad_m = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "        right_curverad_m = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "        # Now our radius of curvature is in meters\n",
    "        \n",
    "        return (left_curverad, right_curverad, left_curverad_m, right_curverad_m)\n",
    "\n",
    "    def process(self, binary_warped):\n",
    "        self.left_lane_inds = []\n",
    "        self.right_lane_inds = []\n",
    "        self.windows = []\n",
    "        \n",
    "        if not self.always_recalculate and self.left_fit is not None and self._lanes_are_reasonable():\n",
    "            binary_warped = self._reuse_fits(binary_warped)\n",
    "        else:\n",
    "            binary_warped = self._calculate_fits(binary_warped)\n",
    "        \n",
    "        self.left_curverad, self.right_curverad, self.left_curverad_m, self.right_curverad_m = \\\n",
    "          self.calculate_curvature_radius(self.left_fit, self.right_fit, self.left_fitx, self.right_fitx, self.ploty, self.xm_per_pix, self.ym_per_pix)\n",
    "        \n",
    "        # Find distance to center by calculating difference of bottom-most section of the lane lines and\n",
    "        # then compare it to image center.\n",
    "        length_of_center_of_lanes = (self.right_fitx[len(self.ploty)-1] - self.left_fitx[len(self.ploty)-1])/2\n",
    "        distance_to_center_of_lanes = length_of_center_of_lanes + self.left_fitx[len(self.ploty)-1]\n",
    "        self.distance_to_center = distance_to_center_of_lanes - (binary_warped.shape[1]/2)\n",
    "        self.distance_to_center_m = self.distance_to_center * self.xm_per_pix\n",
    "        \n",
    "        return binary_warped\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "ip_vid = ImagePipeline(input_color='rgb')\n",
    "\n",
    "src = np.float32([[  100.,   719.],\n",
    "                  [  542.,   470.],\n",
    "                  [  738.,   470.],\n",
    "                  [ 1180.,   719.]])\n",
    "\n",
    "dst = np.float32([[ 120.,  720.],\n",
    "                  [ 120.,    0.],\n",
    "                  [ 1160.,    0.],\n",
    "                  [ 1160.,  720.]])\n",
    "\n",
    "ip_vid.set_perspective(src, dst)\n",
    "ip_vid.calibrate(glob.glob('camera_cal/calibration*.jpg'))\n",
    "# ip_vid.mtx = mtx\n",
    "# ip_vid.dist = dist\n",
    "\n",
    "#     mask[v_cutoff:, :][((s_x >= 25) & (s_x <= 255) &\n",
    "#                         (s_y >= 25) & (s_y <= 255)) |\n",
    "#                        ((grad_mag >= 30) & (grad_mag <= 512) &\n",
    "#                         (grad_dir >= 0.2) & (grad_dir <= 1.)) |\n",
    "#                        (ylw == 255) |\n",
    "#                        (highlights == 255)] = 1\n",
    "\n",
    "# sx    \n",
    "t0 = Threshold(trange=(25, 255))\n",
    "t0.add(Color(['u', 'v', 's']))\n",
    "t0.add(Sobel('x', kernel=3))\n",
    "\n",
    "ip_vid.add(t0)\n",
    "\n",
    "# sy\n",
    "t1 = Threshold(trange=(25, 255))\n",
    "t1.add(Color(['u', 'v', 's']))\n",
    "t1.add(Sobel('y', kernel=3))\n",
    "\n",
    "ip_vid.add(t1)\n",
    "\n",
    "# grad_mag\n",
    "t2 = Threshold(trange=(30, 512))\n",
    "t2.add(Color(['u', 'v', 's']))\n",
    "t2.add(Sobel('x', kernel=3))\n",
    "t2.add(Sobel('y', kernel=3))\n",
    "t2.add(Magnitude())\n",
    "\n",
    "ip_vid.add(t2)\n",
    "\n",
    "# grad_dir\n",
    "t3 = Threshold(trange=(0.2, 1.))\n",
    "t3.add(Color(['u', 'v', 's']))\n",
    "t3.add(Sobel('x', kernel=3))\n",
    "t3.add(Sobel('y', kernel=3))\n",
    "t3.add(Direction())\n",
    "\n",
    "ip_vid.add(t3)\n",
    "\n",
    "# yellow\n",
    "t4 = Threshold()\n",
    "t4.add(Color(['h', 's', 'v'], in_range=((20, 50, 150), (40, 255, 255)), gray=False))\n",
    "\n",
    "ip_vid.add(t4)\n",
    "\n",
    "# highlight\n",
    "t5 = Threshold()\n",
    "t5.add(Color(['r'], in_range=((lambda img: int(np.percentile(img, 99.9) - 30)), 255), gray=False))\n",
    "\n",
    "ip_vid.add(t5)\n",
    "\n",
    "c = Combinator(f=(lambda ths: np.where(\n",
    "    ((ths[0] == 1) & (ths[1] == 1)) |\n",
    "    ((ths[2] == 1) & (ths[3] == 1)) |\n",
    "    (ths[4]) | (ths[5])  \n",
    ")))\n",
    "ip_vid.add(c)\n",
    "f = FindLinesSlidingWindows(always_recalculate=False,\n",
    "                            ok_curverad_diff_m_max=1000,\n",
    "                            alpha=0.1)\n",
    "ip_vid.add(f)\n",
    "\n",
    "a = Annotate(f)\n",
    "ip_vid.add(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# output = 'project_video_processed.mp4'\n",
    "# clip2 = VideoFileClip('project_video.mp4')\n",
    "\n",
    "# ip_vid = ImagePipeline(input_color='rgb')\n",
    "\n",
    "# src = np.float32([[  100.,   719.],\n",
    "#                   [  542.,   470.],\n",
    "#                   [  738.,   470.],\n",
    "#                   [ 1180.,   719.]])\n",
    "\n",
    "# dst = np.float32([[ 120.,  720.],\n",
    "#                   [ 120.,    0.],\n",
    "#                   [ 1160.,    0.],\n",
    "#                   [ 1160.,  720.]])\n",
    "\n",
    "# ip_vid.set_perspective(src, dst)\n",
    "# # ip_final.calibrate(glob.glob('camera_cal/calibration*.jpg'))\n",
    "# ip_vid.mtx = mtx\n",
    "# ip_vid.dist = dist\n",
    "\n",
    "# t1 = Threshold(trange=(11, 137))\n",
    "# t1.add(Color(['s']))\n",
    "# t1.add(Sobel('y', kernel=3))\n",
    "# t1.add(Sobel('x', kernel=3))\n",
    "# t1.add(Magnitude())\n",
    "\n",
    "# ip_vid.add(t1)\n",
    "# c = Combinator(f=(lambda ths: np.where((ths[0] == 1))))\n",
    "# ip_vid.add(c)\n",
    "# f = FindLinesSlidingWindows(always_recalculate=False,\n",
    "#                             ok_curverad_diff_m_max=1000,\n",
    "#                             alpha=0.1)\n",
    "# ip_vid.add(f)\n",
    "\n",
    "# a = Annotate(f)\n",
    "# ip_vid.add(a)\n",
    "\n",
    "# clip = clip2.fl_image(ip_vid.process)\n",
    "# %time clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video project_video_processed.mp4\n",
      "[MoviePy] Writing video project_video_processed.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|| 1260/1261 [08:45<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: project_video_processed.mp4 \n",
      "\n",
      "Wall time: 8min 45s\n"
     ]
    }
   ],
   "source": [
    "output = 'project_video_processed.mp4'\n",
    "clip2 = VideoFileClip('project_video.mp4')\n",
    "clip = clip2.fl_image(ip_vid.process)\n",
    "%time clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video challenge_video_processed.mp4\n",
      "[MoviePy] Writing video challenge_video_processed.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|                                                                             | 20/485 [00:08<03:18,  2.34it/s]"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected non-empty vector for x",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-97989ba659fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mclip2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mVideoFileClip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'challenge_video.mp4'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfl_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mip_vid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'time clip.write_videofile(output, audio=False)'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[1;34m(self, arg_s)\u001b[0m\n\u001b[0;32m   2156\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2158\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2160\u001b[0m     \u001b[1;31m#-------------------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line)\u001b[0m\n\u001b[0;32m   2077\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2078\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2079\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2080\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 188\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    189\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mtime\u001b[1;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[0;32m   1179\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;34m'eval'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1181\u001b[1;33m             \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1182\u001b[0m             \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1183\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<timed eval>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-173>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mrequires_duration\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Attribute 'duration' not set\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-172>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36muse_clip_fps_by_default\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m    135\u001b[0m              for (k,v) in k.items()}\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 137\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<decorator-gen-171>\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mconvert_masks_to_RGB\u001b[1;34m(f, clip, *a, **k)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mismask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_RGB\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36mwrite_videofile\u001b[1;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params)\u001b[0m\n\u001b[0;32m    336\u001b[0m                            \u001b[0maudiofile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maudiofile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    337\u001b[0m                            \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreads\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mthreads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 338\u001b[1;33m                            ffmpeg_params=ffmpeg_params)\n\u001b[0m\u001b[0;32m    339\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    340\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mremove_temp\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmake_audio\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\io\\ffmpeg_writer.py\u001b[0m in \u001b[0;36mffmpeg_write_video\u001b[1;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     for t,frame in clip.iter_frames(progress_bar=True, with_times=True,\n\u001b[1;32m--> 216\u001b[1;33m                                     fps=fps, dtype=\"uint8\"):\n\u001b[0m\u001b[0;32m    217\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mwithmask\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m             \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mclip\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tqdm\\_tqdm.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    831\u001b[0m \"\"\", fp_write=getattr(self.fp, 'write', sys.stderr.write))\n\u001b[0;32m    832\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 833\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    834\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    835\u001b[0m                 \u001b[1;31m# Update and print the progressbar.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36mgenerator\u001b[1;34m()\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mduration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.0\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mfps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m                 \u001b[0mframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-136>\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(self, t)\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(f, *a, **kw)\u001b[0m\n\u001b[0;32m     87\u001b[0m         new_kw = {k: fun(v) if k in varnames else v\n\u001b[0;32m     88\u001b[0m                  for (k,v) in kw.items()}\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnew_a\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mnew_kw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mdecorator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecorator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwrapper\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(self, t)\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 95\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\Clip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[1;31m#mf = copy(self.make_frame)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[0mnewclip\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_make_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mfun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mkeep_duration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\moviepy\\video\\VideoClip.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(gf, t)\u001b[0m\n\u001b[0;32m    511\u001b[0m         \u001b[0;31m`\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mby\u001b[0m \u001b[0manother\u001b[0m \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mimage_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    512\u001b[0m         \"\"\"\n\u001b[1;32m--> 513\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mgf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mimage_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapply_to\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    514\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m     \u001b[1;31m# --------------------------------------------------------------\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\teguh\\Dropbox\\Projects\\learn\\udacity\\nanodegree_self_driving\\p4-CarND-Advanced-Lane-Lines\\image_pipeline.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    554\u001b[0m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mTYPE_LINEAR\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 556\u001b[1;33m                 \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m             \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\teguh\\Dropbox\\Projects\\learn\\udacity\\nanodegree_self_driving\\p4-CarND-Advanced-Lane-Lines\\image_pipeline.py\u001b[0m in \u001b[0;36mprocess\u001b[1;34m(self, binary_warped)\u001b[0m\n\u001b[0;32m    375\u001b[0m             \u001b[0mbinary_warped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reuse_fits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_warped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    376\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 377\u001b[1;33m             \u001b[0mbinary_warped\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_calculate_fits\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbinary_warped\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    378\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_curverad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_curverad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_curverad_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_curverad_m\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\teguh\\Dropbox\\Projects\\learn\\udacity\\nanodegree_self_driving\\p4-CarND-Advanced-Lane-Lines\\image_pipeline.py\u001b[0m in \u001b[0;36m_calculate_fits\u001b[1;34m(self, binary_warped)\u001b[0m\n\u001b[0;32m    284\u001b[0m         \u001b[1;31m# Fit a second order polynomial to each\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleft_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlefty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleftx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 286\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mright_fit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolyfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrighty\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrightx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    287\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    288\u001b[0m         \u001b[1;31m# An array of y value from 0 to (image height - 1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\numpy\\lib\\polynomial.py\u001b[0m in \u001b[0;36mpolyfit\u001b[1;34m(x, y, deg, rcond, full, w, cov)\u001b[0m\n\u001b[0;32m    553\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    554\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 555\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected non-empty vector for x\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    556\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    557\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"expected 1D or 2D array for y\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected non-empty vector for x"
     ]
    }
   ],
   "source": [
    "output = 'challenge_video_processed.mp4'\n",
    "clip2 = VideoFileClip('challenge_video.mp4')\n",
    "clip = clip2.fl_image(ip_vid.process)\n",
    "%time clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "output = 'harder_challenge_video_processed.mp4'\n",
    "clip2 = VideoFileClip('harder_challenge_video.mp4')\n",
    "clip = clip2.fl_image(ip_vid.process)\n",
    "%time clip.write_videofile(output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
